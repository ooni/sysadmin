#!/usr/bin/env python3

doci_version = "2023-11-22.2"

"""
Builds deb packages and uploads them to S3-compatible storage.
Works locally and on GitHub Actions and CircleCI
Detects which package[s] need to be built.
Support "release" and PR/testing archives.

scan              - scan the current repository for packages to be built
build             - locate and build packages
upload <filename> - upload one package to S3
ci                - detect CircleCI PRs, build and upload packages
delete_from_archive <filename> - delete filename from archive

Features:
 - Implement CI/CD workflow using package archives
 - Support adding packages to an existing S3 archive without requiring a
   local mirror
 - Support multiple packages in the same git repository
 - GPG signing
 - Support multiple architectures
 - Update changelogs automatically
 - Easy to debug
 - Phased updates (rolling deployments)
"""

# TODO: fix S3 credentials passing security
# TODO: Phased-Update-Percentage

# debdeps: git
from argparse import ArgumentParser, Namespace
from datetime import datetime, timedelta
import base64
import os
from os import getenv
from pathlib import Path
from subprocess import run
from tempfile import mkdtemp, NamedTemporaryFile
from textwrap import dedent
from time import sleep
from typing import List, Tuple, Dict, Optional
from hashlib import sha256
import sys

try:
    import gnupg
except ImportError:
    gnupg = None

# TODO remove these
DEFAULT_ORG = "ooni"
DEFAULT_PR_REPO = "internal-pull-requests"
DEFAULT_MASTER_REPO = "internal-master"
DEFAULT_REPO = "internal-pull-requests"

EXAMPLE_CONFIG = """
"""

assert sys.version_info >= (3, 7, 0), "Python 3.7.0 or later is required"

conf: Namespace


def getenv_or_raise(name: str) -> str:
    val = getenv(name)
    if val is None:
        raise Exception(f"Missing {name} environment variable")

    return val


def run_always(cmd: str, **kw) -> str:
    if conf.show_commands:
        print(f"Running {cmd}\nKW: {kw}")

    p = run(cmd.split(), capture_output=True, **kw)
    if p.returncode != 0:
        stdout = p.stdout.decode().strip()
        print(f"--stdout--\n{stdout}\n----\n")
        stderr = p.stderr.decode().strip()
        print(f"--stderr--\n{stderr}\n----\n")
        raise Exception(f"'{cmd}' returned: {p.returncode}")
    return p.stdout.decode().strip()

def run2(cmd: str, **kw) -> str:
    """Runs command expecting exit code to be 0.
    Prints out stderr and stdout and raises an exception otherwise.
    """
    if conf.dry_run:
        return
    return run_always(cmd, **kw)

def runi(cmd: str, cwd: Path, sudo=False) -> None:
    if sudo:
        cmd = f"sudo {cmd}"
    if conf.dry_run:
        print(f"DRYRUN: $ {cmd}")
        return
    run(cmd.split(), cwd=cwd, check=True)


def runc(cmd: str) -> Tuple[int, str]:
    if conf.dry_run:
        print(f"DRYRUN: $ {cmd}")
        return
    print("Running:", cmd)
    r = run(cmd.split(), capture_output=True)
    print("Return code:", r.returncode)
    return r.returncode, r.stdout.decode()


def detect_changed_packages() -> List[Path]:
    """Detects files named debian/changelog
    that have been changed in the current branch
    """
    DCH = "debian/changelog"
    # TODO: find a cleaner method:
    commit = run_always("git merge-base remotes/origin/master HEAD")
    changes = run_always(f"git diff --name-status {commit}")
    pkgs = set()
    for cs in changes.splitlines():
        p = cs.split("\t")
        status = p[0]
        path = p[1]
        c = Path(path)
        if c.as_posix().endswith(DCH) and status != "D":
            pkgs.add(c.parent.parent)
            continue
        while c.name:
            if c.joinpath(DCH).is_file():
                pkgs.add(c)
            c = c.parent

    return sorted(pkgs)


def trim_compare(url: str) -> str:
    """Shorten GitHub URLs used to compare changes"""
    if url.startswith("https://github.com/") and "..." in url:
        base, commits = url.rsplit("/", 1)
        if len(commits) == 83:
            beginning = commits[0:8]
            end = commits[43 : 43 + 8]
            return f"{base}/{beginning}...{end}"

    return url


def _set_pkg_version_from_circleci(p: Path, ver: str) -> bool:
    comp = trim_compare(getenv("CIRCLE_COMPARE_URL", ""))  # show changes in VCS
    if not comp:
        # https://discuss.circleci.com/t/circle-compare-url-is-empty/24549/8
        comp = getenv("CIRCLE_PULL_REQUEST") or ""

    if getenv("CIRCLE_PULL_REQUEST"):
        # This is a PR: build ~pr<N>-<N> version. CIRCLE_PR_NUMBER is broken
        pr_num = getenv("CIRCLE_PULL_REQUEST", "").rsplit("/", 1)[-1]
        build_num = getenv("CIRCLE_BUILD_NUM")
        ver = f"{ver}~pr{pr_num}-{build_num}"
        print(f"CircleCI Pull Request detected - using version {ver}")
        run2(f"dch -b -v {ver} {comp}", cwd=p)
        run2(f"dch -r {ver} {comp}", cwd=p)
        ver2 = run2("dpkg-parsechangelog --show-field Version", cwd=p)
        assert ver == ver2, ver + " <--> " + ver2
        return True

    if getenv("CIRCLE_BRANCH") == "master":
        # This is a build outside of a PR and in the mainline branch
        print(f"CircleCI mainline build detected - using version {ver}")
        run2(f"dch -b -v {ver} {comp}", cwd=p)
        run2(f"dch -r {ver} {comp}", cwd=p)
        ver2 = run2("dpkg-parsechangelog --show-field Version", cwd=p)
        assert ver == ver2, ver + " <--> " + ver2
        return True

    # This is a build for a new branch but without a PR: ignore it
    return False


def _set_pkg_version_from_github_actions(p: Path, ver: str) -> bool:
    """When running in GitHub Actions, access env vars to set
    the package version"""
    # GITHUB_REF syntax: refs/heads/<branch-name> or refs/pull/<PR#>/merge
    gh_ref = getenv_or_raise("GITHUB_REF")
    try:
        pr_num = int(gh_ref.split("/")[2])
    except ValueError:
        pr_num = None

    gh_run_number = int(getenv_or_raise("GITHUB_RUN_NUMBER"))
    print(f"GitHub Actions PR #: {pr_num} Run #: {gh_run_number}")
    print("SHA " + getenv_or_raise("GITHUB_SHA"))

    if pr_num is None:
        if gh_ref.endswith("/master"):
            print(f"GitHub release build detected - using version {ver}")
            run2(f"dch -b -v {ver} ''", cwd=p)
            run2("dch --release ''", cwd=p)
            ver2 = run2("dpkg-parsechangelog --show-field Version", cwd=p)
            assert ver == ver2, ver + " <--> " + ver2
            return True

        else:
            print("Not a PR or release build. Skipping.")  # run by "on: push"
            return False

    else:
        # This is a PR: build ~pr<N>-<N> version.
        ver = f"{ver}~pr{pr_num}-{gh_run_number}"
        print(f"GitHub Pull Request detected - using version {ver}")
        run2(f"dch -b -v {ver} ''", cwd=p)
        run2("dch --release ''", cwd=p)
        ver2 = run2("dpkg-parsechangelog --show-field Version", cwd=p)
        assert ver == ver2, ver + " <--> " + ver2
        return True


def buildpkg(p: Path) -> List[Path]:
    """Build one package, installing required dependencies"""
    print(f"Building package in {p}")
    ver = run_always("dpkg-parsechangelog --show-field Version", cwd=p)
    assert ver, f"No version number found in {p}/debian/changelog"
    sudo = True
    should_build = False
    if getenv("CIRCLECI"):
        # Running in CircleCI
        sudo = False
        should_build = _set_pkg_version_from_circleci(p, ver)
    elif getenv("GITHUB_EVENT_PATH"):
        sudo = False
        should_build = _set_pkg_version_from_github_actions(p, ver)

    if not should_build:
        return []

    runi("apt-get build-dep -qy --no-install-recommends .", p, sudo=sudo)
    runi("fakeroot debian/rules build", p)
    runi("fakeroot debian/rules binary", p)
    with p.joinpath("debian/files").open() as f:
        return [p.parent.joinpath(line.split()[0]) for line in f]


def detect_archive_backend() -> Optional[str]:
    if getenv("AWS_ACCESS_KEY_ID") and getenv("AWS_SECRET_ACCESS_KEY"):
        return "s3"

    return None


def setup_gpg_key(
    keyfp: Optional[str], tmpdir: Path
) -> Tuple[gnupg.GPG, Optional[str]]:
    """Import key from env var or use existing keyring"""
    if gnupg is None:
        print("Please install python3-gnupg")
        sys.exit(1)

    if "DEB_GPG_KEY" in os.environ:
        gpg_key_raw = getenv_or_raise("DEB_GPG_KEY")
    elif "DEB_GPG_KEY_BASE64" in os.environ:
        k = getenv_or_raise("DEB_GPG_KEY_BASE64").encode("utf-8")
        gpg_key_raw = base64.b64decode(k)
    else:
        gpg_key_raw = None

    if keyfp is None and gpg_key_raw is None:
        print(
            "Error: place a GPG key in the DEB_GPG_KEY or DEB_GPG_KEY_BASE64"
            " env var or fetch it from the local keyring using --gpg-key-fp"
        )
        sys.exit(1)

    if gpg_key_raw:
        gpg = gnupg.GPG(gnupghome=tmpdir.as_posix())
        import_result = gpg.import_keys(gpg_key_raw)
        assert import_result.count == 1
        fp = import_result.fingerprints[0]
        if keyfp:
            assert keyfp == fp

    else:
        gpg = gnupg.GPG()
        assert gpg.list_keys(keys=keyfp)

    return gpg, keyfp


def ci(args) -> None:
    # TODO: detect sudo presence

    backend_name = detect_archive_backend()
    if backend_name == "s3":
        backend = S3()
    else:
        print("Set AWS_ACCESS_KEY_ID / AWS_SECRET_ACCESS_KEY env vars")
        sys.exit(1)
    del backend_name

    run2("apt-get update -q")
    run2("apt-get install -qy --no-install-recommends git")
    pkgdirs = detect_changed_packages()
    if not pkgdirs:
        print("Nothing to build")
        return

    print(f"Building {pkgdirs}")
    run2("apt-get install -qy --no-install-recommends devscripts")

    pkgs_lists = [buildpkg(pd) for pd in pkgdirs]
    print(f"Processing {pkgs_lists}")
    for pli in pkgs_lists:
        for p in pli:
            backend.upload(p, args)


def build() -> None:
    """Run manual build on workstation"""
    pkgdirs = detect_changed_packages()
    pkgs_lists = [buildpkg(pd) for pd in pkgdirs]
    print("Outputs:")
    for pli in pkgs_lists:
        for p in pli:
            print(p)


class DuplicatePkgError(Exception):
    pass


def extract_pva(text: str):
    d: Dict[str, str] = {}
    for line in text.splitlines():
        if line.startswith("Package: "):
            val = line.split(" ", 1)[1]
            if d:
                yield d
            d = {"p": val}
        elif line.startswith("Version: "):
            d["v"] = line.split(" ", 1)[1]
        elif line.startswith("Architecture: "):
            d["a"] = line.split(" ", 1)[1]

    yield d


def check_duplicate_package(pkgblock: str, packages_text) -> None:
    """Check for already uploaded package: once a package is published its
    contents should not be overwritten.
    """
    new = next(extract_pva(pkgblock))
    existing = list(extract_pva(packages_text))
    if new in existing:
        raise DuplicatePkgError()


def _extract_archs_from_InRelease(content: str) -> dict:
    out = {}
    in_block = False
    for li in content.splitlines():
        if li == "SHA256:":
            in_block = True

        elif in_block and li.startswith(" "):
            sha, size, fn = li.split()
            # Support only main/binary-<arch>/Packages
            if fn.startswith("main/binary-") and fn.endswith("/Packages"):
                arch = fn[12:-9]
                out[arch] = (sha, size)

        elif in_block:
            return out

    return out


class S3:
    """S3 backend"""

    def fetch_and_parse_release_file(self, baseuri, tmpdir):
        """Extract architectures and SHAs"""
        # e.g. s3://ooni-internal-deb/dists/unstable/InRelease
        uri = f"{baseuri}/InRelease"
        oldf = tmpdir / "InRelease.old"
        try:
            run2(f"s3cmd --no-progress get {uri} {oldf.as_posix()}")
            content = oldf.read_text()
        except Exception as e:
            print("Previous InRelease not found.")
            return {}

        return _extract_archs_from_InRelease(content)

    def generate_release_file(self, conf, sha: str, size: int, archd: dict) -> str:
        now = datetime.utcnow()
        dfmt = "%a, %d %b %Y %H:%M:%S UTC"
        date = now.strftime(dfmt)
        until = (now + timedelta(days=3650)).strftime(dfmt)
        archd[conf.arch] = (sha, size)
        archs = " ".join(sorted(archd.keys()))
        r = dedent(
            f"""
            Acquire-By-Hash: no
            Architectures: {archs}
            Codename: {conf.distro}
            Components: main
            Date: {date}
            Origin: {conf.origin}
            Valid-Until: {until}
            SHA256:
        """
        )
        for arch, (sha, size) in sorted(archd.items()):
            line = f" {sha} {size} main/binary-{arch}/Packages\n"
            r += line

        return r

    def init_archive(self, conf):
        """Initialize the archive"""
        assert conf.bucket_name
        r, o = runc(f"s3cmd mb s3://{conf.bucket_name}")
        if r == 0:
            print("Bucket created")
            runc(f"s3cmd ws-create s3://{conf.bucket_name}")

        s3url = None
        r, out = runc(f"s3cmd ws-info s3://{conf.bucket_name}")
        for li in out.splitlines():
            if li.startswith("Website endpoint"):
                s3url = li.split()[2]
                break
        assert s3url

        # Initialize distro if needed. Check for InRelease
        baseuri = f"s3://{conf.bucket_name}/dists/{conf.distro}"
        r, o = runc(f"s3cmd info --no-progress {baseuri}/InRelease")
        if r == 0:
            return

        if r != 12:
            print(f"Unexpected return code {r} {o}")
            sys.exit(1)

        # InRelease file not found: create lock file
        print("Creating initial lock file")
        tf = NamedTemporaryFile()
        # put = "s3cmd --acl-public --guess-mime-type --no-progress put"
        put = "s3cmd --guess-mime-type --no-progress put"
        r2, o = runc(f"{put} --no-progress {tf.name} {baseuri}/.debrepos3.lock")
        assert r2 == 0, repr(o)

        # Create empty InRelease
        r2, o = runc(f"{put} {tf.name} {baseuri}/InRelease")

        # Create empty Packages
        r, o = runc(f"{put} {tf.name} {baseuri}/main/binary-{conf.arch}/Packages")

        # Create index
        html = dedent(
            f"""
            <html><body>
            <p>Create /etc/apt/sources.list.d/{conf.distro}.list containing:</p>
            <pre>deb {s3url} {conf.distro} main</pre>
            </body></html>
        """
        )
        with open(tf.name, "w") as f:
            f.write(html)

        r, o = runc(f"{put} {tf.name} {baseuri}/index.html")

    def lock(self, conf, baseuri: str) -> None:
        """Rename semaphore file"""
        print(f"Locking {baseuri} ...")
        cmd = f"s3cmd mv --no-progress {baseuri}/.debrepos3.nolock {baseuri}/.debrepos3.lock"
        while True:
            r, o = runc(cmd)
            print(r)
            if r == 0:
                return

            print("The distro is locked. Waiting...")
            sleep(10)

    def unlock(self, baseuri: str) -> None:
        """Rename semaphore file"""
        r, o = runc(
            f"s3cmd mv --no-progress {baseuri}/.debrepos3.lock {baseuri}/.debrepos3.nolock"
        )
        print(r)

    def scanpackages(self, conf, debfn) -> str:
        lines = run2(f"dpkg-scanpackages {debfn}")
        out = []
        for line in lines.splitlines():
            if line.startswith("Filename: "):
                fn = line.split("/")[-1]
                line = f"Filename: dists/{conf.distro}/main/binary-{conf.arch}/{fn}"
            out.append(line)

        return "\n".join(out) + "\n"

    def _inner_upload(
        self, debfn, tmpdir, baseuri, pkgblock: str, gpg, gpgkeyfp
    ) -> None:
        # Fetch existing Packages file
        packages = tmpdir / "Packages"
        assert conf
        uri = f"{baseuri}/main/binary-{conf.arch}/Packages {packages}"
        run2(f"s3cmd --no-progress get {uri}")

        # Check for already uploaded package
        check_duplicate_package(pkgblock, packages.read_text())

        # Append, then read whole file back: this is meant to add a new block
        # without any risk of altering the existing content
        with packages.open("a") as f:
            # Explicitly add newlines for safety
            pkgblock = "\n" + pkgblock.strip() + "\n"
            f.write(pkgblock)

        data = packages.read_bytes()
        packagesf_size = len(data)
        packagesf_sha = sha256(data).hexdigest()
        del data

        # Create, sign, upload InRelease
        release = tmpdir / "Release"
        inrelease = tmpdir / "InRelease"
        archd = self.fetch_and_parse_release_file(baseuri, tmpdir)
        rfdata = self.generate_release_file(conf, packagesf_sha, packagesf_size, archd)
        release.write_text(rfdata)
        # r, o = runc(f"gpg -a -s --clearsign -o {inrelease} {release}")
        # if r != 0:
        #    self.unlock(baseuri)
        #    print("Error during GPG signature")
        #    sys.exit(1)
        sig = gpg.sign(release.read_text(), keyid=gpgkeyfp)
        assert sig.status == "signature created"
        inrelease.write_bytes(sig.data)

        # Upload InRelease and Packages
        put = "s3cmd --acl-public --guess-mime-type --no-progress put"
        run2(f"{put} {inrelease} {baseuri}/InRelease")
        run2(f"{put} {packages} {baseuri}/main/binary-{conf.arch}/Packages")
        run2(f"{put} {debfn} {baseuri}/main/binary-{conf.arch}/")

    def upload(self, debfn: Path, conf) -> None:
        assert conf.bucket_name
        tmpdir = Path(mkdtemp(prefix="debops-ci"))

        self.init_archive(conf)
        baseuri = f"s3://{conf.bucket_name}/dists/{conf.distro}"

        pkgblock = self.scanpackages(conf, debfn)

        gpg, gpgkeyfp = setup_gpg_key(conf.gpg_key_fp, tmpdir)

        # Lock distro on S3 to prevent race during appends to Packages
        self.lock(conf, baseuri)

        try:
            self._inner_upload(debfn, tmpdir, baseuri, pkgblock, gpg, gpgkeyfp)
        except DuplicatePkgError:
            print(f"Error: {debfn} is already in the archive. Not uploading.")
            sys.exit(1)  # the unlock in the finally block is still executed
        finally:
            self.unlock(baseuri)

    #  # TODO check
    #  dpkg-scanpackages $1 | gzip >> Packages.gz
    #  upload Packages.gz
    #  rm Packages.gz


def main() -> None:
    global conf
    print(f"debops-ci version {doci_version}")
    ap = ArgumentParser(usage=__doc__)
    ap.add_argument(
        "action", choices=("upload", "scan", "ci", "build", "delete_from_archive")
    )
    ap.add_argument("--bucket-name", help="S3 bucket name")
    ap.add_argument("--distro", default="unstable", help="Debian distribution name")
    ap.add_argument("--dry-run", action="store_true", help="Debian distribution name")
    ap.add_argument("--origin", default="private", help="Debian Origin name")
    ap.add_argument("--arch", default="amd64", help="Debian architecture name")
    ap.add_argument("--gpg-key-fp", help="GPG key fingerprint")
    ap.add_argument("--show-commands", action="store_true", help="Show shell commands")
    args, extra = ap.parse_known_args()
    conf = args

    if args.action == "ci":
        ci(args)
    elif args.action == "scan":
        for p in sorted(detect_changed_packages()):
            print(p.as_posix())
    elif args.action == "upload":
        bk = S3()
        for fn in extra:
            bk.upload(Path(fn), args)
    elif args.action == "delete_from_archive":
        raise NotImplementedError
        # bk = S3()
        # bk.delete_package(args, extra)
    elif args.action == "build":
        build()


if __name__ == "__main__":
    main()
